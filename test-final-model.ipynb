{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a90df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:53:35.211239: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 35s 34ms/step - loss: 2.0973 - accuracy: 0.3979 - val_loss: 0.8824 - val_accuracy: 0.7163\n",
      "Epoch 2/50\n",
      "980/980 [==============================] - 57s 58ms/step - loss: 0.9069 - accuracy: 0.7138 - val_loss: 0.3949 - val_accuracy: 0.8780\n",
      "Epoch 3/50\n",
      "980/980 [==============================] - 78s 80ms/step - loss: 0.5301 - accuracy: 0.8356 - val_loss: 0.1957 - val_accuracy: 0.9433\n",
      "Epoch 4/50\n",
      "102/980 [==>...........................] - ETA: 1:02 - loss: 0.4328 - accuracy: 0.8664"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = 'gtsrb-dataset'\n",
    "\n",
    "# Load CSV files\n",
    "train_data = pd.read_csv(os.path.join(dataset_path, 'Train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(dataset_path, 'Test_Images.csv'))\n",
    "meta_data = pd.read_csv(os.path.join(dataset_path, 'Meta.csv'))\n",
    "\n",
    "# Preprocess a single image\n",
    "def preprocess_image(image_path, class_id):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((30, 30))  # Resize the image to (30, 30)\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")  # Convert grayscale images to RGB\n",
    "    image_array = np.array(image)\n",
    "    if len(image_array.shape) == 2:  # Handle grayscale images\n",
    "        image_array = np.stack((image_array,) * 3, axis=-1)\n",
    "    image_array = image_array / 255.0  # Normalize the image\n",
    "    return image_array, class_id\n",
    "\n",
    "# Load and preprocess training images\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for i, row in train_data.iterrows():\n",
    "    image_path = os.path.join(dataset_path, row['Path'])\n",
    "    image, class_id = preprocess_image(image_path, row['ClassId'])\n",
    "    train_images.append(image)\n",
    "    train_labels.append(class_id)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform one-hot encoding on the training labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "train_labels = label_binarizer.fit_transform(train_labels)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=123)\n",
    "\n",
    "# Load and preprocess test images\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i, row in test_data.iterrows():\n",
    "    image_path = os.path.join(dataset_path, row['Path'])\n",
    "    image, class_id = preprocess_image(image_path, row['ClassId'])\n",
    "    test_images.append(image)\n",
    "    test_labels.append(class_id)\n",
    "\n",
    "# Convert test_images to a NumPy array\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Create the convolutional neural network model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(30, 30, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(43, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define data augmentation configuration\n",
    "data_augmentation = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define learning rate schedule\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with data augmentation and learning rate scheduling\n",
    "model.fit(data_augmentation.flow(train_images, train_labels, batch_size=32),\n",
    "          epochs=50,\n",
    "          steps_per_epoch=len(train_images) // 32,\n",
    "          validation_data=(valid_images, valid_labels),\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# Convert the test labels to one-hot format\n",
    "test_labels = label_binarizer.transform(test_labels)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the test images\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert one-hot encoded test labels back to their original class labels\n",
    "true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Display the classified test images\n",
    "fig, axes = plt.subplots(6, 4, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(24):\n",
    "    axes[i].imshow(test_images[i])\n",
    "    axes[i].set_title(f\"True Class: {true_labels[i]}\\nPredicted Class: {predicted_labels[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(test_labels, axis=1), predicted_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf87142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe445d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
